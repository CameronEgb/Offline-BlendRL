SYSTEM OVERVIEW - BLENDERL REFACTOR
====================================

This document describes the workflow and architectural integration of the refactored 
Gemini CLI / BlendeRL framework.

1. CONFIGURATION & ENTRY POINT
------------------------------
- Script: ./run_full_cycle.sh
- Purpose: The primary entry point. You edit the variables at the top of this script 
  (EXPERIMENT_ID, ENVIRONMENT, etc.) to define your experiment.
- Workflow:
  1. The shell script exports these variables to the environment.
  2. It calls run_full_cycle.py.
  3. run_full_cycle.py reads these variables and orchestrates the submission of 
     online and offline jobs.

2. ONLINE TRAINING (DATA COLLECTION)
------------------------------------
- Scripts: train_neuralppo.py, train_blenderl.py
- Integration:
  - Both scripts now support a --run_id flag, allowing the orchestrator to force a 
    specific directory name (e.g., "ppo_exp001").
  - Dataset Generation: If --save_dataset is passed, the scripts use DatasetWriter 
    (from dataset_utils.py) to save transitions (s, a, r, s', d) into pkl chunks.
  - Data Location: Transitions are saved to offline_dataset/{run_id}/.
  - Model/Log Location: Checkpoints and Tensorboard logs are saved to out/runs/{run_id}/.

3. OFFLINE TRAINING (IQL & INTERVALS)
-------------------------------------
- Scripts: train_iql.py, train_blendrl_iql.py
- Integration:
  - These scripts implement Implicit Q-Learning (IQL). 
  - train_blendrl_iql.py specifically trains the hybrid BlendRL agent using 
    the IQL loss on the offline dataset.
- Interval Training Logic:
  - The orchestrator calculates data limits based on INTERVALS_COUNT.
  - Each interval, the DatasetReader (dataset_utils.py) restricts its sampling 
    range to simulate a growing dataset.
  - The model trains for OFFLINE_EPOCHS on that specific subset.
  - After each interval, the model is evaluated in a live environment, and 
    the result is saved to results.json.

4. JOB ORCHESTRATION
--------------------
- Script: run_full_cycle.py
- Local Mode (LOCAL=true): Starts background processes and saves logs to 
  logs/{EXPERIMENT_ID}/{job_name}.log.
- Cluster Mode (LOCAL=false): Generates and submits sbatch scripts.
- Dependency Management: Offline jobs are submitted with a dependency on the 
  online data collection job. They only begin once the online job finishes 
  successfully.

5. ANALYSIS & UTILITIES
-----------------------
- plot_results.py: 
  - Scans out/runs/ for folders matching the EXPERIMENT_ID.
  - Parses training_log.pkl for online performance.
  - Parses results.json for offline interval performance.
  - Generates PNG plots in the plots/ directory.
- kill_experiments.sh: 
  - Uses scancel and pkill to stop all jobs/processes tied to an experiment ID.
- sync_logs.sh: 
  - Rsyncs results from the remote cluster to your local out/runs/ folder.

6. DATA STRUCTURE SUMMARY
-------------------------
- out/runs/{run_id}/: Checkpoints, Tensorboard logs, training_log.pkl, results.json.
- offline_dataset/{run_id}/: Transition pkl chunks.
- plots/{exp_id}/: Generated performance graphs.
- logs/{exp_id}/: Stdout/Stderr logs from batch jobs.
